import warnings
warnings.filterwarnings("ignore")
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
# Prompts and retriever
from langchain.prompts import PromptTemplate
from langchain.vectorstores import DeepLake
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereEmbeddings, \
    CohereRerank as langchain_cohere_reranker
import os
import time
from dotenv import load_dotenv

# loading .env
load_dotenv('.env')


# The Prompt
def prompt():
    prompt_template_researcher = """
    As an NLP researcher, Give an in depth explanation for your the text the given text below in a brief manner. 
    If the text is a straightforward answer, be brief and simply answer without any form  deep explanation\n
    For example:
    Who authored the paper with Daniel Ihenacho?
    List of co-authors:
     - Naruto Uzumaki
     - David Dike
     - Victor Peters
     - Jerry Peters
     - Bobby M. The First
    If the answer is not in the resource, vector database. Simply reply, I do not know the answer to this questionğŸ‘€
    Text: {text}
    Your answer should be 
    {text}\n\n\n

    Full description:
    """
    prompt_researcher_input_variables = ['text']
    prompt_researcher = PromptTemplate(
        template=prompt_template_researcher,
        input_variables=prompt_researcher_input_variables
    )
    return prompt_researcher

# Creating Vector Database
def vector_database(cohere_key, my_activeloop_org_id, my_activeloop_dataset_name, activeloop_token):
    cohere_embeddings = CohereEmbeddings(
        model="embed-english-v3.0",
        cohere_api_key=cohere_key
    )

    dataset_path = f"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}"
    # Pass activeloop_token if it's needed for authentication with the hub.
    try:
        db = DeepLake(dataset_path=dataset_path, embedding=cohere_embeddings, read_only=True, token=activeloop_token)
        return db
    except ConnectionError as e:
        print(f"Error: {e}\nCannot connect to deeplake")

# Setting up the base retriever
def base_retriever(db):
    search_kwargs = {"k":4} # Top 4 retrieved docs
    retriever = db.as_retriever(search_kwargs=search_kwargs)
    return retriever

# Setting up the Reranker
def reranker_retriever(retriever):
    compressor = langchain_cohere_reranker(model='rerank-english-v3.0')
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,base_retriever=retriever
    )
    return compression_retriever

# Retrieving question, final answer and source docs
def compressed_docs(compression_retriever, question:str):
    compressed_docs = compression_retriever.invoke(question)
    retrieved_chunks = [doc.page_content for doc in compressed_docs]
    final_answer = "\n\n".join(retrieved_chunks).strip()
    return question, final_answer, compressed_docs

# The Final Researcher answers
def final_formatted_answer(prompt_researcher,final_answer,compressed_docs,chat_model):
    researcher_input_data = {"text":final_answer.strip()}
    llm_researcher_chain = prompt_researcher | chat_model
    researcher_response = llm_researcher_chain.invoke(researcher_input_data)
    print(f"Researcher answer:\n{researcher_response.content}")
    try:
        sources = []
        for i in range(len(compressed_docs)):
            source = compressed_docs[i].metadata.get('source', 'Unknown Source')
            if source not in sources:
                sources.append(source)
    except IndexError as error:
        print("No sources")
        sources.append("No sources")

    print("Source(s):")
    for source in sources:
        print(f"- {source}")

# The LLM model
def llm_model(huggingface_token):
    repo_id = "mistralai/Mistral-7B-Instruct-v0.3"
    model_kwargs = {
        "temperature": 0.1, 
        "timeout": 6000,
    }
    llm = HuggingFaceEndpoint(
        repo_id=repo_id,
        huggingfacehub_api_token = huggingface_token,
        **model_kwargs,
    )
    chat_model = ChatHuggingFace(llm=llm)
    return chat_model

# Environment checker
def get_env_key(enivronment_name, env_key):
    value = os.getenv(env_key)
    if value:
        print(f"{env_key} found in environment.")
    else:
        value = input(f"{env_key} not found. Please enter your {enivronment_name}: ")
    return value

# Main loop
while True:
    try:
        print("\nHi there! This is Ugomma,\nI'm your research assistant and here to help! ğŸ˜Š")
        time.sleep(3)
        print("But let me check your environment first.\n")

        # Collect keys and IDs
        COHERE_API_KEY = get_env_key("Cohere API Key", "COHERE_API_KEY")
        ACTIVELOOP_TOKEN = get_env_key("Activeloop Token", "ACTIVELOOP_TOKEN")
        HUGGINGFACEHUB_API_TOKEN = get_env_key("Huggingface token", "HUGGINGFACEHUB_API_TOKEN")
        
        # Attempt initial connections to validate credentials
        if not COHERE_API_KEY:
            raise ValueError("Cohere API Key is missing.")
        if not ACTIVELOOP_TOKEN:
            raise ValueError("Activeloop Token is missing.")
        if not HUGGINGFACEHUB_API_TOKEN:
            raise ValueError("Huggingface token is missing.")

        my_activeloop_org_id = input("Your Active Loop ID > ").strip()
        my_activeloop_dataset_name = input("Active Loop Dataset Name > ").strip()

        if not my_activeloop_org_id or not my_activeloop_dataset_name:
            raise ValueError("Active Loop Organization ID and Dataset Name are required.")


        print("\nAttempting to connect to services...")
        time.sleep(2)
        print("\nâœ… You're all set and connected!")
        
        # Calling all functions
        chat_model = llm_model(HUGGINGFACEHUB_API_TOKEN)
        prompt_researcher = prompt()
        db = vector_database(COHERE_API_KEY, my_activeloop_org_id, my_activeloop_dataset_name, ACTIVELOOP_TOKEN)
        retriever = base_retriever(db)
        reranker = reranker_retriever(retriever)

        # Asking for questions
        while True:
            time.sleep(2)
            question = input("\nAsk me anything (or type 'no' to exit) > ").strip().lower()
            if question == 'no':
                confirm_exit = input("Are you sure you want to exit? (yes/no): ").strip().lower()
                if confirm_exit == 'yes':
                    print("Goodbye for now! ğŸ™‚")
                    exit()
                continue

            try:
                # Catching any unexpected Error
                time.sleep(3)
                print("Almost there!")
                question, final_answer, compressed_docs_ = compressed_docs(reranker, question)
                # Final Answers
                final_formatted_answer(prompt_researcher, final_answer, compressed_docs_, chat_model)
            except Exception as e:
                print(f"\nâŒ An unexpected error occurred: \n{e}\n")
                print("Something went wrong during question processing. Please try again.")
                leave = input("Want to leave? Type 'exit' or 'leave'. Otherwise, press Enter > ").strip().lower()
                if leave in ['exit', 'leave']:
                    print("Exiting program. Bye ğŸ™‚")
                    exit()
                continue
            
            # Do you want to Exit the program?
            stop = input("\nType 'STOP' to exit or press Enter to ask another question: ").strip().upper()
            if stop == "STOP":
                print("See you next time! ğŸ‘‹")
                exit()

    # General Exceptions
    except Exception as general_err:
        print(f"\nâŒ A critical error occurred during application setup: {general_err}\n")
        print("The program cannot continue. Please review your setup and try again.")
        exit()